{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/detect-sarcasm-in-comments/sample_submission.csv\n/kaggle/input/detect-sarcasm-in-comments/Test.csv\n/kaggle/input/detect-sarcasm-in-comments/Train.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"###### Load the data.."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(r'../input/detect-sarcasm-in-comments/Train.csv')\ntest_data = pd.read_csv(r'../input/detect-sarcasm-in-comments/Test.csv')\nprint(\"train size: \", train_data.shape)\nprint(\"test size: \", test_data.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"train size:  (15000, 10)\ntest size:  (8000, 9)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"           ID                                            comment     date  \\\n0  uid_590555  Well, let's be honest here, they don't actuall...  2015-04   \n1  uid_671762  Well, I didn't need evidence to believe in com...  2016-12   \n2  uid_519689              Who does an \"official promo\" in 360p?  2013-11   \n3  uid_788362                           Grotto koth was the best  2015-09   \n4  uid_299252                                   Neal's back baby  2015-11   \n\n   down                                     parent_comment  score  top  \\\n0     0  They should shut the fuck up and let the commu...      2    2   \n1    -1  You need evidence to kill people? I thought we...      6   -1   \n2     0                    2014 BMW S1000R: Official Promo      3    3   \n3     0  Not really that memorable lol if you want memo...      2    2   \n4     0                      James Neal hit on Zach Parise     -5   -5   \n\n              topic            user  label  \n0       starcitizen  Combat_Wombatz      0  \n1  EnoughCommieSpam        starkadd      1  \n2       motorcycles         phybere      0  \n3        hcfactions          m0xyMC      1  \n4            hockey       Somuch101      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>comment</th>\n      <th>date</th>\n      <th>down</th>\n      <th>parent_comment</th>\n      <th>score</th>\n      <th>top</th>\n      <th>topic</th>\n      <th>user</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>uid_590555</td>\n      <td>Well, let's be honest here, they don't actuall...</td>\n      <td>2015-04</td>\n      <td>0</td>\n      <td>They should shut the fuck up and let the commu...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>starcitizen</td>\n      <td>Combat_Wombatz</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>uid_671762</td>\n      <td>Well, I didn't need evidence to believe in com...</td>\n      <td>2016-12</td>\n      <td>-1</td>\n      <td>You need evidence to kill people? I thought we...</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>EnoughCommieSpam</td>\n      <td>starkadd</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>uid_519689</td>\n      <td>Who does an \"official promo\" in 360p?</td>\n      <td>2013-11</td>\n      <td>0</td>\n      <td>2014 BMW S1000R: Official Promo</td>\n      <td>3</td>\n      <td>3</td>\n      <td>motorcycles</td>\n      <td>phybere</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>uid_788362</td>\n      <td>Grotto koth was the best</td>\n      <td>2015-09</td>\n      <td>0</td>\n      <td>Not really that memorable lol if you want memo...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>hcfactions</td>\n      <td>m0xyMC</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>uid_299252</td>\n      <td>Neal's back baby</td>\n      <td>2015-11</td>\n      <td>0</td>\n      <td>James Neal hit on Zach Parise</td>\n      <td>-5</td>\n      <td>-5</td>\n      <td>hockey</td>\n      <td>Somuch101</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Looks like classes are balanced'''\ntrain_data[\"label\"].value_counts()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"1    7527\n0    7473\nName: label, dtype: int64"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#!pip install --upgrade transformers\n!pip install transformers==3.1.0","execution_count":5,"outputs":[{"output_type":"stream","text":"Collecting transformers==3.1.0\n  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n\u001b[K     |████████████████████████████████| 884 kB 3.0 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (3.0.10)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (20.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (0.0.43)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (1.18.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (4.45.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (2.23.0)\nCollecting tokenizers==0.8.1.rc2\n  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |████████████████████████████████| 3.0 MB 19.9 MB/s eta 0:00:01     |███▎                            | 307 kB 19.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (2020.4.4)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.1.0) (0.1.91)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.1.0) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3.1.0) (2.4.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.1.0) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3.1.0) (7.1.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.1.0) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.1.0) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.1.0) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3.1.0) (2020.6.20)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.7.0\n    Uninstalling tokenizers-0.7.0:\n      Successfully uninstalled tokenizers-0.7.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 2.11.0\n    Uninstalling transformers-2.11.0:\n      Successfully uninstalled transformers-2.11.0\n\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\nallennlp 1.0.0 requires transformers<2.12,>=2.9, but you'll have transformers 3.1.0 which is incompatible.\u001b[0m\nSuccessfully installed tokenizers-0.8.1rc2 transformers-3.1.0\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport fastai\nfrom fastai.text import *\nfrom fastai.metrics import *\nimport transformers\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaForSequenceClassification","execution_count":6,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"transformers version: \", transformers.__version__)\nprint(\"fast.ai version: \", fastai.__version__)","execution_count":7,"outputs":[{"output_type":"stream","text":"transformers version:  3.1.0\nfast.ai version:  1.0.61\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### FastAI wrapper methods around Roberta Tokenizer "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    task = \"sarcasm-detection\",\n    testing=False,\n    seed = 101,\n    roberta_model_name='roberta-large', # can also be exchanged with roberta-base \n    max_lr=1e-5,\n    epochs=3,\n    use_fp16=False,\n    bs=4, \n    max_seq_len=512, \n    num_labels = 2,\n    hidden_dropout_prob=.05,\n    hidden_size=768, # 1024 for roberta-large model\n    start_tok = \"<s>\",\n    end_tok = \"</s>\",\n    mark_fields=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FastAiRobertaTokenizer(BaseTokenizer):\n    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        return [\"<s>\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"</s>\"]\n    \n    \nclass RobertaTokenizeProcessor(TokenizeProcessor):\n    def __init__(self, tokenizer):\n        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n         \nclass RobertaNumericalizeProcessor(NumericalizeProcessor):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, vocab=fastai_vocab, **kwargs)\n        \ndef get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]\n\n\nclass RobertaDataBunch(TextDataBunch):\n    @classmethod\n    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n        \n        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n        val_bs = ifnone(val_bs, bs)\n        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n        dataloaders = [train_dl]\n        for ds in datasets[1:]:\n            lengths = [len(t) for t in ds.x.items]\n            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n    \n\nclass RobertaTextList(TextList):\n    _bunch = RobertaDataBunch\n    _label_cls = TextList","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Prepare the train & valid data "},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Prepare pandas DF'''\nfeat_cols = \"combined_comments\"\nlabel_cols = \"label\"\n\ntrain_data[\"combined_comments\"] = train_data[\"parent_comment\"] + \" </s> </s> \" + train_data[\"comment\"]\ntest_data[\"combined_comments\"] = test_data[\"parent_comment\"] + \" </s> </s> \" + test_data[\"comment\"]\n\ntrain_data[label_cols] = train_data[label_cols].astype(int)\n\n''' Creating validation data '''\nX_train, X_val, y_train, y_val = train_test_split(train_data[feat_cols], train_data[label_cols], \\\n                                                  test_size=0.02, random_state=99)\n\ntrain_data = pd.concat([X_train, y_train], axis = 1)\nval_data = pd.concat([X_val, y_val], axis = 1)\n\n\nprint(\"train shape: \", len(train_data))\nprint(\"val shape: \", len(val_data))\nprint(\"test shape: \", len(test_data))","execution_count":10,"outputs":[{"output_type":"stream","text":"train shape:  14700\nval shape:  300\ntest shape:  8000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"###### Look at random samples to understand the comments "},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = np.random.randint(0, len(train_data), 1)\nprint(\"text: \", train_data.iloc[idx][feat_cols].values)\nprint(\"\\n\")\nprint(\"label: \", train_data.iloc[idx][label_cols])","execution_count":11,"outputs":[{"output_type":"stream","text":"text:  ['OMG! What a save! </s> </s> Close one!']\n\n\nlabel:  9722    0\nName: label, dtype: int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Initialize Roberta tokenizer'''\nroberta_tok = RobertaTokenizer.from_pretrained(config.roberta_model_name)\n\nfastai_tokenizer = Tokenizer(tok_func = FastAiRobertaTokenizer(roberta_tok, \\\n                                                               max_seq_len=config.max_seq_len), \\\n                             pre_rules=[], post_rules=[])\n\n''' Construct fast.ai vocab using RobertaTokenizer dictionary'''\npath = Path()\nroberta_tok.save_vocabulary(path)\nwith open('vocab.json', 'r') as f:\n    roberta_vocab_dict = json.load(f)\n    \nfastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))\nprint(\"Roberta dict size: \", len(roberta_vocab_dict.keys()))\n\n\nprint(\"Batch size is : \", config.bs)\n# loading the tokenizer and vocab processors\nprocessor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n\n# creating our databunch \ndata = ItemLists(\".\", RobertaTextList.from_df(train_data, \".\", cols=feat_cols, processor=processor),\n                      RobertaTextList.from_df(val_data, \".\", cols=feat_cols, processor=processor)\n                ) \\\n       .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n       .add_test(RobertaTextList.from_df(test_data, \".\", cols=feat_cols, processor=processor)) \\\n       .databunch(bs=config.bs,pad_first=False)","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450a8ef2cad04d9da6ab03f9459426a1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a3d72aa4ba74f76911c6579c5cb2dba"}},"metadata":{}},{"output_type":"stream","text":"\nRoberta dict size:  50265\nBatch size is :  4\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"###### Model training "},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining our model architecture \nclass RobertaForSequenceClassificationModel(nn.Module):\n    def __init__(self,num_labels=config.num_labels):\n        super(RobertaForSequenceClassificationModel,self).__init__()\n        self.num_labels = num_labels\n        self.roberta = RobertaForSequenceClassification.from_pretrained(config.roberta_model_name,\\\n                                                                        num_labels= self.num_labels)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        outputs = self.roberta(input_ids, token_type_ids, attention_mask)\n        logits = outputs[0] \n        return logits","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_model = RobertaForSequenceClassificationModel(config.num_labels) \nlearn = Learner(data, roberta_model, metrics=[accuracy])\nlearn.model.roberta.train() # setting roberta to train as it is in eval mode by default","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=482.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0060c37adf045d981399911749ff767"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1425941629.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab245eb3de344048a83962381fc7ad5c"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (12): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (13): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (14): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (15): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (16): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (17): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (18): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (19): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (20): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (21): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (22): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (23): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"###### Plot the LR against loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/1 00:00<00:00]\n    </div>\n    \n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='81' class='' max='3675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      2.20% [81/3675 00:18<13:23 2.4474]\n    </div>\n    "},"metadata":{}},{"output_type":"stream","text":"LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cuda available: \", torch.cuda.is_available())\n# Looks like 2 epochs are enough on Large transformers on this data\n# 3 is kind of overfitting\nlearn.fit_one_cycle(3, max_lr=1e-6)","execution_count":16,"outputs":[{"output_type":"stream","text":"Cuda available:  True\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.628611</td>\n      <td>0.735646</td>\n      <td>0.573333</td>\n      <td>11:25</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.508020</td>\n      <td>0.594300</td>\n      <td>0.700000</td>\n      <td>11:26</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.501764</td>\n      <td>0.573047</td>\n      <td>0.726667</td>\n      <td>11:26</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"###### Inference on Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type, p_learn) -> np.ndarray:\n    p_learn.model.roberta.eval()\n    preds = p_learn.get_preds(ds_type)[0].detach().cpu().numpy()\n    \n    sampler = [i for i in data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    ordered_preds = preds[reverse_sampler, :]\n    pred_values = np.argmax(ordered_preds, axis=1)\n    return ordered_preds, pred_values","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Predict validation accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# val preds\npreds, pred_values = get_preds_as_nparray(DatasetType.Valid, learn)\nacc = (pred_values == data.valid_ds.y.items).mean()\nprint(\"Validation accuracy: \", acc)","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"stream","text":"Validation accuracy:  0.7266666666666667\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Compute test predictions and save to csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ids = []\npreds = []\n''' Eval mode'''\nlearn.model.roberta.eval()\nfor idx, row in test_data.iterrows():\n    comment = row[\"combined_comments\"]\n    u_id = row[\"ID\"]\n    \n    prediction = torch.argmax(learn.predict(comment)[2]).item()\n    #print(\"Prediction: \", prediction)\n    #print(\"\\n\")\n    if idx % 100 == 0:\n        print(\"test_idx: \", idx)\n        \n    user_ids.append(u_id)\n    preds.append(prediction)","execution_count":19,"outputs":[{"output_type":"stream","text":"test_idx:  0\ntest_idx:  100\ntest_idx:  200\ntest_idx:  300\ntest_idx:  400\ntest_idx:  500\ntest_idx:  600\ntest_idx:  700\ntest_idx:  800\ntest_idx:  900\ntest_idx:  1000\ntest_idx:  1100\ntest_idx:  1200\ntest_idx:  1300\ntest_idx:  1400\ntest_idx:  1500\ntest_idx:  1600\ntest_idx:  1700\ntest_idx:  1800\ntest_idx:  1900\ntest_idx:  2000\ntest_idx:  2100\ntest_idx:  2200\ntest_idx:  2300\ntest_idx:  2400\ntest_idx:  2500\ntest_idx:  2600\ntest_idx:  2700\ntest_idx:  2800\ntest_idx:  2900\ntest_idx:  3000\ntest_idx:  3100\ntest_idx:  3200\ntest_idx:  3300\ntest_idx:  3400\ntest_idx:  3500\ntest_idx:  3600\ntest_idx:  3700\ntest_idx:  3800\ntest_idx:  3900\ntest_idx:  4000\ntest_idx:  4100\ntest_idx:  4200\ntest_idx:  4300\ntest_idx:  4400\ntest_idx:  4500\ntest_idx:  4600\ntest_idx:  4700\ntest_idx:  4800\ntest_idx:  4900\ntest_idx:  5000\ntest_idx:  5100\ntest_idx:  5200\ntest_idx:  5300\ntest_idx:  5400\ntest_idx:  5500\ntest_idx:  5600\ntest_idx:  5700\ntest_idx:  5800\ntest_idx:  5900\ntest_idx:  6000\ntest_idx:  6100\ntest_idx:  6200\ntest_idx:  6300\ntest_idx:  6400\ntest_idx:  6500\ntest_idx:  6600\ntest_idx:  6700\ntest_idx:  6800\ntest_idx:  6900\ntest_idx:  7000\ntest_idx:  7100\ntest_idx:  7200\ntest_idx:  7300\ntest_idx:  7400\ntest_idx:  7500\ntest_idx:  7600\ntest_idx:  7700\ntest_idx:  7800\ntest_idx:  7900\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data.head()\n#np.mean(np.array(targets) == np.array(preds))\npd.DataFrame({'ID': user_ids, 'label': preds}).to_csv(\"submission.csv\", index=False)","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Save the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"#learn.save(\"roberta_large_tuned_sarcasm_model\")","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"0    4018\n1    3982\nName: label, dtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"###### Load the saved model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}